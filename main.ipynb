{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing_extensions import Any, Dict, List, Tuple, Union\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchsummary import summary\n",
    "from tqdm_loggable.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmToLogger(io.StringIO):\n",
    "    def __init__(self, logger, level=logging.INFO):\n",
    "        super().__init__()\n",
    "        self.logger = logger\n",
    "        self.level = level\n",
    "\n",
    "    def write(self, buf):\n",
    "        # strip() removes the \\r and \\n characters tqdm uses for terminal updates\n",
    "        content = buf.strip('\\r\\n\\t ')\n",
    "        if content:\n",
    "            self.logger.log(self.level, content)\n",
    "\n",
    "    def flush(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b79965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger():\n",
    "    logger = logging.getLogger(\"main_logger\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Clear existing handlers if any (prevents duplicate logs in notebooks)\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Create a format for your logs\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Stream Handler (Standard Output)\n",
    "    sh = logging.StreamHandler(sys.stdout)\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(sh)\n",
    "\n",
    "    # File Handler (Direct logging to a file)\n",
    "    fh = logging.FileHandler(\"experiment_progress.log\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = get_logger()\n",
    "tqdm_out = TqdmToLogger(logger, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e12b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "generator = torch.Generator()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed: int = SEED) -> None:\n",
    "    # set random seed for reproducibility\n",
    "    logger.info(f\"Setting seed: {seed}\")\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    generator.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = None\n",
    "fashion_mnist_dataset = None\n",
    "train_split_ratio = 0.7\n",
    "val_split_ratio = 0.1\n",
    "test_split_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b220b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset() -> None:\n",
    "    logger.info(\"Loading MNIST dataset...\")\n",
    "    global mnist_dataset\n",
    "    \n",
    "    if mnist_dataset is None:\n",
    "        train_dataset = torchvision.datasets.MNIST(\n",
    "            root='./data',\n",
    "            train=True,\n",
    "            transform=torchvision.transforms.ToTensor(),\n",
    "            download=True\n",
    "        )\n",
    "        test_dataset = torchvision.datasets.MNIST(\n",
    "            root='./data',\n",
    "            train=False,\n",
    "            transform=torchvision.transforms.ToTensor(),\n",
    "            download=True\n",
    "        )\n",
    "        \n",
    "        # concatenate train and test datasets\n",
    "        mnist_dataset = torch.utils.data.ConcatDataset(\n",
    "            [\n",
    "                train_dataset,\n",
    "                test_dataset\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # split into train, val, test\n",
    "        total_size = len(mnist_dataset)\n",
    "        train_size = int(train_split_ratio * total_size)\n",
    "        val_size = int(val_split_ratio * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        mnist_train, mnist_val, mnist_test = torch.utils.data.random_split(\n",
    "            mnist_dataset,\n",
    "            [train_size, val_size, test_size],\n",
    "            generator=generator\n",
    "        )\n",
    "        \n",
    "        mnist_dataset = {\n",
    "            \"train\": mnist_train,\n",
    "            \"val\": mnist_val,\n",
    "            \"test\": mnist_test\n",
    "        }\n",
    "        \n",
    "        del train_dataset, test_dataset, mnist_train, mnist_val, mnist_test\n",
    "\n",
    "\n",
    "def load_fashion_mnist_dataset() -> None:\n",
    "    logger.info(\"Loading Fashion-MNIST dataset...\")\n",
    "    global fashion_mnist_dataset\n",
    "    \n",
    "    if fashion_mnist_dataset is None:\n",
    "        train_dataset = torchvision.datasets.FashionMNIST(\n",
    "            root='./data',\n",
    "            train=True,\n",
    "            transform=torchvision.transforms.ToTensor(),\n",
    "            download=True\n",
    "        )\n",
    "        test_dataset = torchvision.datasets.FashionMNIST(\n",
    "            root='./data',\n",
    "            train=False,\n",
    "            transform=torchvision.transforms.ToTensor(),\n",
    "            download=True\n",
    "        )\n",
    "        # concatenate train and test datasets\n",
    "        fashion_mnist_dataset = torch.utils.data.ConcatDataset(\n",
    "            [\n",
    "                train_dataset,\n",
    "                test_dataset\n",
    "            ]\n",
    "        )\n",
    "        # split into train, val, test\n",
    "        total_size = len(fashion_mnist_dataset)\n",
    "        train_size = int(train_split_ratio * total_size)\n",
    "        val_size = int(val_split_ratio * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        fashion_mnist_train, fashion_mnist_val, fashion_mnist_test = torch.utils.data.random_split(\n",
    "            fashion_mnist_dataset,\n",
    "            [train_size, val_size, test_size],\n",
    "            generator=generator\n",
    "        )\n",
    "        \n",
    "        fashion_mnist_dataset = {\n",
    "            \"train\": fashion_mnist_train,\n",
    "            \"val\": fashion_mnist_val,\n",
    "            \"test\": fashion_mnist_test\n",
    "        }\n",
    "        \n",
    "        del train_dataset, test_dataset, fashion_mnist_train, fashion_mnist_val, fashion_mnist_test\n",
    "\n",
    "def load_datasets() -> None:\n",
    "    load_mnist_dataset()\n",
    "    load_fashion_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b36ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset information\n",
    "logger.info(f\"MNIST train data size - {len(mnist_dataset['train'])}\")\n",
    "logger.info(f\"MNIST train data size - {len(mnist_dataset['val'])}\")\n",
    "logger.info(f\"MNIST train data size - {len(mnist_dataset['test'])}\")\n",
    "logger.info(f\"MNIST image shape - {mnist_dataset['train'][0][0].shape}\\n\")\n",
    "\n",
    "logger.info(f\"Fashion-MNIST train data size - {len(fashion_mnist_dataset['train'])}\")\n",
    "logger.info(f\"Fashion-MNIST train data size - {len(fashion_mnist_dataset['val'])}\")\n",
    "logger.info(f\"Fashion-MNIST train data size - {len(fashion_mnist_dataset['test'])}\")\n",
    "logger.info(f\"Fashion-MNIST image shape - {fashion_mnist_dataset['train'][0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(256),\n",
    "    T.RandomResizedCrop(\n",
    "        224,\n",
    "        scale=(0.8, 1.0),\n",
    "        ratio=(0.9, 1.1)\n",
    "    ),\n",
    "    T.RandomRotation(15),\n",
    "    T.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1)\n",
    "    ),\n",
    "    T.Grayscale(num_output_channels=3),  # 3 channel for resnet\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(224),\n",
    "    T.Grayscale(num_output_channels=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79cc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_dataset: torch.utils.data.Dataset,\n",
    "        transform: Union[torchvision.transforms.Compose, None] = None\n",
    "    ):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Any:\n",
    "        img, label = self.base_dataset[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8511c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_mnist_dataset = None\n",
    "aug_fashion_mnist_dataset = None\n",
    "\n",
    "def load_augmented_datasets() -> None:\n",
    "    logger.info(\"Loading augmented datasets...\")\n",
    "    global aug_mnist_dataset, aug_fashion_mnist_dataset\n",
    "    \n",
    "    if aug_mnist_dataset is None:\n",
    "        aug_mnist_dataset = {\n",
    "            \"train\": customDataset(\n",
    "                mnist_dataset[\"train\"],\n",
    "                transform=train_transform\n",
    "            ),\n",
    "            \"val\": customDataset(\n",
    "                mnist_dataset[\"val\"],\n",
    "                transform=test_transform\n",
    "            ),\n",
    "            \"test\": customDataset(\n",
    "                mnist_dataset[\"test\"],\n",
    "                transform=test_transform\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    if aug_fashion_mnist_dataset is None:\n",
    "        aug_fashion_mnist_dataset = {\n",
    "            \"train\": customDataset(\n",
    "                fashion_mnist_dataset[\"train\"],\n",
    "                transform=train_transform\n",
    "            ),\n",
    "            \"val\": customDataset(\n",
    "                fashion_mnist_dataset[\"val\"],\n",
    "                transform=test_transform\n",
    "            ),\n",
    "            \"test\": customDataset(\n",
    "                fashion_mnist_dataset[\"test\"],\n",
    "                transform=test_transform\n",
    "            )\n",
    "        }\n",
    "\n",
    "load_augmented_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset information\n",
    "logger.info(f\"MNIST train data size - {len(aug_mnist_dataset['train'])}\")\n",
    "logger.info(f\"MNIST train data size - {len(aug_mnist_dataset['val'])}\")\n",
    "logger.info(f\"MNIST train data size - {len(aug_mnist_dataset['test'])}\")\n",
    "logger.info(f\"MNIST image shape - {aug_mnist_dataset['train'][0][0].shape}\\n\")\n",
    "\n",
    "logger.info(f\"Fashion-MNIST train data size - {len(aug_fashion_mnist_dataset['train'])}\")\n",
    "logger.info(f\"Fashion-MNIST train data size - {len(aug_fashion_mnist_dataset['val'])}\")\n",
    "logger.info(f\"Fashion-MNIST train data size - {len(aug_fashion_mnist_dataset['test'])}\")\n",
    "logger.info(f\"Fashion-MNIST image shape - {aug_fashion_mnist_dataset['train'][0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18_model(num_classes: int = 10) -> torch.nn.Module:\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    # Modify the final layer to match num_classes\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def get_resnet32_model(num_classes: int = 10) -> torch.nn.Module:\n",
    "    model = torchvision.models.resnet34(pretrained=False)\n",
    "    # Modify the final layer to match num_classes\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def get_resnet50_model(num_classes: int = 10) -> torch.nn.Module:\n",
    "    model = torchvision.models.resnet50(pretrained=False)\n",
    "    # Modify the final layer to match num_classes\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f268b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"ResNet18 Model Summary:\")\n",
    "summary(get_resnet18_model().to(device), (3, 224, 224)) # ~11 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"ResNet32 Model Summary:\")\n",
    "summary(get_resnet32_model().to(device), (3, 224, 224)) # ~21 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39267ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"ResNet50 Model Summary:\")\n",
    "summary(get_resnet50_model().to(device), (3, 224, 224)) # ~25 million parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36cfa77",
   "metadata": {},
   "source": [
    "### Q1, a) Training RESNET-18, 50 on MNIST, Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d6adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(\n",
    "    dataset: torch.utils.data.Dataset,\n",
    "    batch_size: int,\n",
    "    drop_last: bool = False,\n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 8\n",
    ") -> torch.utils.data.DataLoader:\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        generator=generator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss and accuracy\n",
    "def calculate_loss_and_accuracy(\n",
    "    model: torch.nn.Module,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels).item()\n",
    "            total_samples += inputs.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f195688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_plots(\n",
    "    training_stats: Dict[str, list],\n",
    "    save_dir: str\n",
    ") -> None:\n",
    "    epochs = range(1, len(training_stats[\"train_loss\"]) + 1)\n",
    "    \n",
    "    # Plot Loss\n",
    "    # x-axis should be intiger\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(epochs, training_stats[\"train_loss\"], label='Train Loss')\n",
    "    plt.plot(epochs, training_stats[\"val_loss\"], label='Validation Loss')\n",
    "    # plt.plot(epochs, training_stats[\"test_loss\"], label='Test Loss')\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(save_dir, 'loss_plot.png')\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, training_stats[\"train_accuracy\"], label='Train Accuracy')\n",
    "    plt.plot(epochs, training_stats[\"val_accuracy\"], label='Validation Accuracy')\n",
    "    # plt.plot(epochs, training_stats[\"test_accuracy\"], label='Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    acc_plot_path = os.path.join(save_dir, 'accuracy_plot.png')\n",
    "    plt.savefig(acc_plot_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa4b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_stats(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    config: Dict[str, Any],\n",
    "    training_stats: Dict[str, list],\n",
    "    base_dir: str    \n",
    ") -> Tuple[str, str, str]:\n",
    "    # model_save_dir_name = (\n",
    "    #     f\"model_{config['dataset_name']}_{config['model_name']}_{config['optimizer_name']}_\",\n",
    "    #     f\"lr_{str(config['learning_rate']).replace('.', '_')}_batch{config['batch_size']}_epoch{epoch+1}\"\n",
    "    # )\n",
    "    model_save_dir_name = '_'.join(f'{key}_{value}' for key, value in config.items())\n",
    "    model_dir = os.path.join(base_dir, ''.join(model_save_dir_name))\n",
    "    os.makedirs(model_dir, exist_ok=True) \n",
    "    \n",
    "    model_path = os.path.join(model_dir, 'model.pth')\n",
    "    model_save_state = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'training_stats': training_stats,\n",
    "        'config': config\n",
    "    }\n",
    "    torch.save(model_save_state, model_path)\n",
    "    \n",
    "    del model_save_state\n",
    "    return model_path, model_dir, base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e964ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @torch.compile(fullgraph=True)\n",
    "def train_model(config: Dict[str, Any], base_dir: str, device:torch.device) -> List[Dict[str, Any]]:\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    # config has keys: dataset_name, model_name, learning_rate, batch_size, optimizer_name, num_epochs\n",
    "    if config[\"model_name\"] == \"resnet18\":\n",
    "        model = get_resnet18_model().to(device)\n",
    "    elif config[\"model_name\"] == \"resnet32\":\n",
    "        model = get_resnet32_model().to(device)\n",
    "    elif config[\"model_name\"] == \"resnet50\":\n",
    "        model = get_resnet50_model().to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model name: {config['model_name']}\")\n",
    "\n",
    "    # model = torch.compile(model)\n",
    "    \n",
    "    if config[\"optimizer_name\"] == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=config[\"learning_rate\"],\n",
    "            momentum=0.9\n",
    "        )\n",
    "    elif config[\"optimizer_name\"] == \"adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config[\"learning_rate\"]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer name: {config['optimizer_name']}\")\n",
    "    \n",
    "    train_data_loader = get_data_loader(\n",
    "        aug_mnist_dataset[\"train\"] if config[\"dataset_name\"] == \"mnist\" else aug_fashion_mnist_dataset[\"train\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_data_loader = get_data_loader(\n",
    "        aug_mnist_dataset[\"val\"] if config[\"dataset_name\"] == \"mnist\" else aug_fashion_mnist_dataset[\"val\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    test_data_loader = get_data_loader(\n",
    "        aug_mnist_dataset[\"test\"] if config[\"dataset_name\"] == \"mnist\" else aug_fashion_mnist_dataset[\"test\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    training_stats = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": []\n",
    "    }\n",
    "    scaler = torch.amp.GradScaler(device=device.type, enabled=(device.type==\"cuda\"))\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    return_data = []\n",
    "    \n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in tqdm(\n",
    "            train_data_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\", file=tqdm_out,\n",
    "            mininterval=30.0\n",
    "        ):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            # with torch.autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            # loss.backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs.float(), 1)\n",
    "            correct_predictions += torch.sum(preds == labels).item()\n",
    "            total_samples += inputs.size(0)\n",
    "            \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_accuracy = correct_predictions / total_samples\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_metrics = calculate_loss_and_accuracy(\n",
    "                model,\n",
    "                val_data_loader,\n",
    "                criterion,\n",
    "                device\n",
    "            )\n",
    "        model.train()\n",
    "        \n",
    "        # test_metrics = calculate_loss_and_accuracy(\n",
    "        #     model,\n",
    "        #     test_data_loader,\n",
    "        #     criterion,\n",
    "        #     device\n",
    "        # )\n",
    "        \n",
    "        training_stats[\"train_loss\"].append(epoch_loss)\n",
    "        training_stats[\"train_accuracy\"].append(epoch_accuracy)\n",
    "        training_stats[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        training_stats[\"val_accuracy\"].append(val_metrics[\"accuracy\"])\n",
    "        # training_stats[\"test_loss\"].append(test_metrics[\"loss\"])\n",
    "        # training_stats[\"test_accuracy\"].append(test_metrics[\"accuracy\"])\n",
    "        \n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        logger.info(\n",
    "            f\"Epoch [{epoch+1}/{config['num_epochs']}], \"\n",
    "            f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_accuracy:.4f}, \"\n",
    "            f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}, \"\n",
    "            # f\"Test Loss: {test_metrics['loss']:.4f}, Test Acc: {test_metrics['accuracy']:.4f}\"\n",
    "        )\n",
    "        \n",
    "        # save intermediate model at every 5 epochs or final epoch\n",
    "        if (epoch + 1)%5==0 or epoch+1==config[\"num_epochs\"]:\n",
    "            mid_train_time = time.time()\n",
    "            mid_elapsed_time = mid_train_time - start_time\n",
    "            logger.info(f\"Mid Training Time after {epoch+1} epochs: {mid_elapsed_time/60:.2f} minutes.\")\n",
    "            \n",
    "            mid_stats = copy.deepcopy(training_stats)\n",
    "            mid_config = copy.deepcopy(config)\n",
    "            mid_config[\"num_epochs\"] = epoch + 1\n",
    "            \n",
    "            mid_model_path, mid_model_dir, _ = save_model_and_stats(\n",
    "                model,\n",
    "                optimizer,\n",
    "                mid_config,\n",
    "                mid_stats,\n",
    "                base_dir=base_dir\n",
    "            )\n",
    "            save_training_plots(mid_stats, mid_model_dir)\n",
    "            logger.info(f\"Mid-training model saved at: {mid_model_path}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mid_test_metrics = calculate_loss_and_accuracy(\n",
    "                    model,\n",
    "                    test_data_loader,\n",
    "                    criterion,\n",
    "                    device\n",
    "                )\n",
    "            model.train()\n",
    "            \n",
    "            mid_stats[\"model_path\"] = mid_model_path\n",
    "            mid_stats[\"model_dir\"] = mid_model_dir\n",
    "            mid_stats[\"base_dir\"] = base_dir\n",
    "            mid_stats[\"training_time\"] = mid_elapsed_time # in seconds\n",
    "            mid_stats[\"test_loss\"] = mid_test_metrics[\"loss\"]\n",
    "            mid_stats[\"test_accuracy\"] = mid_test_metrics[\"accuracy\"]\n",
    "            \n",
    "            return_data.append({**mid_config, **mid_stats})\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    logger.info(f\"Training completed in {elapsed_time/60:.2f} minutes.\")\n",
    "    \n",
    "    del model, optimizer, train_data_loader, val_data_loader, test_data_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64515954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to search for both mnist and fashion-mnist datasets for every configuration\n",
    "base_experiment_dir = './q1_a_experiments'\n",
    "os.makedirs(base_experiment_dir, exist_ok=True)\n",
    "\n",
    "mnist_search_space = {\n",
    "    \"dataset_name\": \"mnist\",\n",
    "    \"model_name\": [\"resnet18\", \"resnet50\"],\n",
    "    \"learning_rate\": [0.001, 0.0001],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"optimizer_name\": [\"sgd\", \"adam\"],\n",
    "    \"num_epochs\": [10]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name in mnist_search_space[\"model_name\"]:\n",
    "    for learning_rate in mnist_search_space[\"learning_rate\"]:\n",
    "        for batch_size in mnist_search_space[\"batch_size\"]:\n",
    "            for optimizer_name in mnist_search_space[\"optimizer_name\"]:\n",
    "                for num_epochs in mnist_search_space[\"num_epochs\"]:\n",
    "                    config = {\n",
    "                        \"dataset_name\": mnist_search_space[\"dataset_name\"],\n",
    "                        \"model_name\": model_name,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"optimizer_name\": optimizer_name,\n",
    "                        \"num_epochs\": num_epochs\n",
    "                    }\n",
    "                    logger.info(f\"Training with config: {config}\")\n",
    "                    return_data = train_model(\n",
    "                        config, os.path.join(base_experiment_dir, 'mnist_experiments'),\n",
    "                        device=device\n",
    "                    )\n",
    "                    logger.info(f\"Training completed.\\n\")\n",
    "                    results.extend(return_data)\n",
    "\n",
    "# save the results to json file\n",
    "result_path = os.path.join(base_experiment_dir, 'mnist_experiments_results.json')\n",
    "with open(result_path, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# identify best model based on test accuracy\n",
    "best_model = max(results, key=lambda x: x['test_accuracy'])\n",
    "logger.info(f\"Best Model Config: {best_model}\")\n",
    "logger.info(f\"Best Model Test Accuracy: {best_model['test_accuracy']:.4f}\")\n",
    "\n",
    "# save best model to base_dir/mnist_best_model.pth\n",
    "best_model_src_path = best_model['model_path']\n",
    "best_model_dest_path = os.path.join(base_experiment_dir, 'mnist_best_model.pth')\n",
    "shutil.copy(best_model_src_path, best_model_dest_path)\n",
    "logger.info(f\"Best model saved to: {best_model_dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b746c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_search_space = {\n",
    "    \"dataset_name\": \"fashion-mnist\",\n",
    "    \"model_name\": [\"resnet18\", \"resnet50\"],\n",
    "    \"learning_rate\": [0.001, 0.0001],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"optimizer_name\": [\"sgd\", \"adam\"],\n",
    "    \"num_epochs\": [10]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name in fashion_mnist_search_space[\"model_name\"]:\n",
    "    for learning_rate in fashion_mnist_search_space[\"learning_rate\"]:\n",
    "        for batch_size in fashion_mnist_search_space[\"batch_size\"]:\n",
    "            for optimizer_name in fashion_mnist_search_space[\"optimizer_name\"]:\n",
    "                for num_epochs in fashion_mnist_search_space[\"num_epochs\"]:\n",
    "                    config = {\n",
    "                        \"dataset_name\": fashion_mnist_search_space[\"dataset_name\"],\n",
    "                        \"model_name\": model_name,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"optimizer_name\": optimizer_name,\n",
    "                        \"num_epochs\": num_epochs\n",
    "                    }\n",
    "                    logger.info(f\"Training with config: {config}\")\n",
    "                    return_data = train_model(\n",
    "                        config, os.path.join(base_experiment_dir, 'fashion_mnist_experiments'),\n",
    "                        device=device\n",
    "                    )\n",
    "                    logger.info(f\"Training completed.\\n\")\n",
    "                    results.extend(return_data)\n",
    "\n",
    "# save the results to json file\n",
    "result_path = os.path.join(base_experiment_dir, 'fashion_mnist_experiments_results.json')\n",
    "with open(result_path, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# identify best model based on test accuracy\n",
    "best_model = max(results, key=lambda x: x['test_accuracy'])\n",
    "logger.info(f\"Best Model Config: {best_model}\")\n",
    "logger.info(f\"Best Model Test Accuracy: {best_model['test_accuracy']:.4f}\")\n",
    "\n",
    "# save best model to base_dir/mnist_best_model.pth\n",
    "best_model_src_path = best_model['model_path']\n",
    "best_model_dest_path = os.path.join(base_experiment_dir, 'fashion_mnist_best_model.pth')\n",
    "shutil.copy(best_model_src_path, best_model_dest_path)\n",
    "logger.info(f\"Best model saved to: {best_model_dest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad9d8a",
   "metadata": {},
   "source": [
    "### Q1, b) Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_mnist_dataset = None\n",
    "numpy_fashion_mnist_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numpy_dataset(\n",
    "    basedata: torch.utils.data.Dataset,\n",
    "    transform: torchvision.transforms.Compose\n",
    "    # n_augments: int = 1\n",
    ") -> Tuple[np.array, np.array]:\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for img, label in tqdm(basedata, desc=\"Creating numpy dataset\", file=tqdm_out, mininterval=30.0):\n",
    "        img_np = transform(img).numpy().reshape(-1)\n",
    "        X_list.append(img_np)\n",
    "        y_list.append(int(label))\n",
    "    \n",
    "    X = np.stack(X_list)\n",
    "    y = np.array(y_list)\n",
    "    del X_list, y_list\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ecaef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_np_mnist_dataset() -> None:\n",
    "    global numpy_mnist_dataset\n",
    "    \n",
    "    train_transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.RandomRotation(15),\n",
    "        T.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "\n",
    "    test_transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    if numpy_mnist_dataset is not None:\n",
    "        return\n",
    "    \n",
    "    train_data = create_numpy_dataset(\n",
    "        basedata=mnist_dataset[\"train\"],\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_data = create_numpy_dataset(\n",
    "        basedata=mnist_dataset[\"val\"],\n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    test_data = create_numpy_dataset(\n",
    "        basedata=mnist_dataset[\"test\"],\n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    numpy_mnist_dataset = {\n",
    "        \"train\": train_data,\n",
    "        \"val\": val_data,\n",
    "        \"test\": test_data\n",
    "    }\n",
    "    return\n",
    "\n",
    "\n",
    "def load_np_fashion_mnist_dataset() -> None:\n",
    "    global numpy_fashion_mnist_dataset\n",
    "    \n",
    "    train_transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.RandomRotation(15),\n",
    "        T.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "\n",
    "    test_transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    if numpy_fashion_mnist_dataset is not None:\n",
    "        return\n",
    "    \n",
    "    train_data = create_numpy_dataset(\n",
    "        basedata=fashion_mnist_dataset[\"train\"],\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_data = create_numpy_dataset(\n",
    "        basedata=fashion_mnist_dataset[\"val\"],\n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    test_data = create_numpy_dataset(\n",
    "        basedata=fashion_mnist_dataset[\"test\"],\n",
    "        transform=test_transform\n",
    "    )\n",
    "    numpy_fashion_mnist_dataset = {\n",
    "        \"train\": train_data,\n",
    "        \"val\": val_data,\n",
    "        \"test\": test_data\n",
    "    }\n",
    "    return\n",
    "\n",
    "def load_np_dataset() -> None:\n",
    "    load_np_mnist_dataset()\n",
    "    load_np_fashion_mnist_dataset()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c21bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_np_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4cd7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_svm_model_and_stats(\n",
    "    model: svm.SVC,\n",
    "    config: Dict[str, Any],\n",
    "    training_stats: Dict[str, Any],\n",
    "    base_dir: str    \n",
    ") -> Tuple[str, str, str]:\n",
    "    model_save_dir_name = '_'.join(f'{key}_{value}' for key, value in config.items())\n",
    "    model_dir = os.path.join(base_dir, ''.join(model_save_dir_name))\n",
    "    os.makedirs(model_dir, exist_ok=True) \n",
    "    \n",
    "    model_path = os.path.join(model_dir, 'svm_model.pkl')\n",
    "    joblib.dump(model, model_path)\n",
    "    \n",
    "    with open(os.path.join(model_dir, 'training_stats.json'), 'w') as f:\n",
    "        json.dump(training_stats, f, indent=4)\n",
    "    \n",
    "    return (model_path, model_dir, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(config: Dict[str, Any], base_dir: str) -> Dict[str, Any]:\n",
    "    # config has keys: dataset_name, kernel_name, kernel_params\n",
    "    if config[\"dataset_name\"] == \"mnist\":\n",
    "        dataset = numpy_mnist_dataset\n",
    "    elif config[\"dataset_name\"] == \"fashion_mnist\":\n",
    "        dataset = numpy_fashion_mnist_dataset\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset name: {config['dataset_name']}\")\n",
    "    \n",
    "    X_train, y_train = dataset[\"train\"]\n",
    "    X_val, y_val = dataset[\"val\"]\n",
    "    X_test, y_test = dataset[\"test\"]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    logger.info(f\"Training SVM with config: {config}\")\n",
    "    \n",
    "    if config[\"kernel_name\"] == \"poly\":\n",
    "        model = svm.SVC(\n",
    "            kernel='poly',\n",
    "            degree=config[\"kernel_params\"].get(\"degree\", 3),\n",
    "            C=config[\"kernel_params\"].get(\"C\", 1.0),\n",
    "            gamma=config[\"kernel_params\"].get(\"gamma\", 'scale'),\n",
    "            random_state=SEED\n",
    "        )\n",
    "    elif config[\"kernel_name\"] == \"rbf\":\n",
    "        model = svm.SVC(\n",
    "            kernel='rbf',\n",
    "            C=config[\"kernel_params\"].get(\"C\", 1.0),\n",
    "            gamma=config[\"kernel_params\"].get(\"gamma\", 'scale'),\n",
    "            random_state=SEED\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported kernel name: {config['kernel_name']}\")\n",
    "    \n",
    "    start_time = time.time()    \n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    logger.info(\n",
    "        f\"SVM Training completed. \"\n",
    "        f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, Test Acc: {test_accuracy:.4f}\"\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"SVM Training completed in {elapsed_time/60:.2f} minutes.\")\n",
    "    \n",
    "    training_stats = {\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"test_accuracy\": test_accuracy\n",
    "    }\n",
    "    \n",
    "    model_bath, model_dir, _ = save_svm_model_and_stats(\n",
    "        model,\n",
    "        config,\n",
    "        training_stats,\n",
    "        base_dir=base_dir\n",
    "    )\n",
    "    \n",
    "    training_stats[\"model_path\"] = model_bath\n",
    "    training_stats[\"model_dir\"] = model_dir\n",
    "    training_stats[\"base_dir\"] = base_dir\n",
    "    training_stats[\"training_time\"] = elapsed_time # in seconds\n",
    "    \n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bbd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_experiment_dir = './q1_b_experiments'\n",
    "os.makedirs(base_experiment_dir, exist_ok=True)\n",
    "\n",
    "mnist_search_space = {\n",
    "    \"dataset_name\": [\"mnist\"],\n",
    "    \"kernel_name\": [\"poly\", \"rbf\"],\n",
    "    \"kernel_params\": {\n",
    "        \"poly\": {\n",
    "            \"degree\": [2, 3],\n",
    "            \"C\": [0.1, 1.0],\n",
    "            \"gamma\": ['scale', 'auto']\n",
    "        },\n",
    "        \"rbf\": {\n",
    "            \"C\": [0.1, 1.0],\n",
    "            \"gamma\": ['scale', 'auto']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset_name in mnist_search_space[\"dataset_name\"]:\n",
    "    for kernel_name in mnist_search_space[\"kernel_name\"]:\n",
    "        if kernel_name == \"poly\":\n",
    "            for degree in mnist_search_space[\"kernel_params\"][\"poly\"][\"degree\"]:\n",
    "                for C in mnist_search_space[\"kernel_params\"][\"poly\"][\"C\"]:\n",
    "                    for gamma in mnist_search_space[\"kernel_params\"][\"poly\"][\"gamma\"]:\n",
    "                        config = {\n",
    "                            \"dataset_name\": dataset_name,\n",
    "                            \"kernel_name\": kernel_name,\n",
    "                            \"kernel_params\": {\n",
    "                                \"degree\": degree,\n",
    "                                \"C\": C,\n",
    "                                \"gamma\": gamma\n",
    "                            }\n",
    "                        }\n",
    "                        training_stats = train_svm(config, os.path.join(base_experiment_dir, 'mnist_svm_experiments'))\n",
    "                        results.append({**config, **training_stats})\n",
    "        elif kernel_name == \"rbf\":\n",
    "            for C in mnist_search_space[\"kernel_params\"][\"rbf\"][\"C\"]:\n",
    "                for gamma in mnist_search_space[\"kernel_params\"][\"rbf\"][\"gamma\"]:\n",
    "                    config = {\n",
    "                        \"dataset_name\": dataset_name,\n",
    "                        \"kernel_name\": kernel_name,\n",
    "                        \"kernel_params\": {\n",
    "                            \"C\": C,\n",
    "                            \"gamma\": gamma\n",
    "                        }\n",
    "                    }\n",
    "                    training_stats = train_svm(config, os.path.join(base_experiment_dir, 'mnist_svm_experiments'))\n",
    "                    results.append({**config, **training_stats})\n",
    "\n",
    "# save the results to json file\n",
    "with open(os.path.join(base_experiment_dir, 'mnist_svm_experiments_results.json'), 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# identify best model based on test accuracy\n",
    "best_model = max(results, key=lambda x: x['test_accuracy'])\n",
    "logger.info(f\"Best SVM Model Config: {best_model}\")\n",
    "logger.info(f\"Best SVM Model Test Accuracy: {best_model['test_accuracy']:.4f}\")\n",
    "\n",
    "# save best model to base_dir/mnist_svm_best_model.pkl\n",
    "best_model_src_path = best_model['model_path']\n",
    "best_model_dest_path = os.path.join(base_experiment_dir, 'mnist_svm_best_model.pkl')\n",
    "shutil.copy(best_model_src_path, best_model_dest_path)\n",
    "logger.info(f\"Best SVM model saved to: {best_model_dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_search_space = {\n",
    "    \"dataset_name\": [\"fashion_mnist\"],\n",
    "    \"kernel_name\": [\"poly\", \"rbf\"],\n",
    "    \"kernel_params\": {\n",
    "        \"poly\": {\n",
    "            \"degree\": [2, 3],\n",
    "            \"C\": [0.1, 1.0],\n",
    "            \"gamma\": ['scale', 'auto']\n",
    "        },\n",
    "        \"rbf\": {\n",
    "            \"C\": [0.1, 1.0],\n",
    "            \"gamma\": ['scale', 'auto']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset_name in fashion_mnist_search_space[\"dataset_name\"]:\n",
    "    for kernel_name in fashion_mnist_search_space[\"kernel_name\"]:\n",
    "        if kernel_name == \"poly\":\n",
    "            for degree in fashion_mnist_search_space[\"kernel_params\"][\"poly\"][\"degree\"]:\n",
    "                for C in fashion_mnist_search_space[\"kernel_params\"][\"poly\"][\"C\"]:\n",
    "                    for gamma in fashion_mnist_search_space[\"kernel_params\"][\"poly\"][\"gamma\"]:\n",
    "                        config = {\n",
    "                            \"dataset_name\": dataset_name,\n",
    "                            \"kernel_name\": kernel_name,\n",
    "                            \"kernel_params\": {\n",
    "                                \"degree\": degree,\n",
    "                                \"C\": C,\n",
    "                                \"gamma\": gamma\n",
    "                            }\n",
    "                        }\n",
    "                        training_stats = train_svm(config, os.path.join(base_experiment_dir, 'fashion_mnist_svm_experiments'))\n",
    "                        results.append({**config, **training_stats})\n",
    "        elif kernel_name == \"rbf\":\n",
    "            for C in fashion_mnist_search_space[\"kernel_params\"][\"rbf\"][\"C\"]:\n",
    "                for gamma in fashion_mnist_search_space[\"kernel_params\"][\"rbf\"][\"gamma\"]:\n",
    "                    config = {\n",
    "                        \"dataset_name\": dataset_name,\n",
    "                        \"kernel_name\": kernel_name,\n",
    "                        \"kernel_params\": {\n",
    "                            \"C\": C,\n",
    "                            \"gamma\": gamma\n",
    "                        }\n",
    "                    }\n",
    "                    training_stats = train_svm(config, os.path.join(base_experiment_dir, 'fashion_mnist_svm_experiments'))\n",
    "                    results.append({**config, **training_stats})\n",
    "\n",
    "# save the results to json file\n",
    "with open(os.path.join(base_experiment_dir, 'fashion_mnist_svm_experiments_results.json'), 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# identify best model based on test accuracy\n",
    "best_model = max(results, key=lambda x: x['test_accuracy'])\n",
    "logger.info(f\"Best SVM Model Config: {best_model}\")\n",
    "logger.info(f\"Best SVM Model Test Accuracy: {best_model['test_accuracy']:.4f}\")\n",
    "\n",
    "# save best model to base_dir/fashion_mnist_svm_best_model.pkl\n",
    "best_model_src_path = best_model['model_path']\n",
    "best_model_dest_path = os.path.join(base_experiment_dir, 'fashion_mnist_svm_best_model.pkl')\n",
    "shutil.copy(best_model_src_path, best_model_dest_path)\n",
    "logger.info(f\"Best SVM model saved to: {best_model_dest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5dad0f",
   "metadata": {},
   "source": [
    "### Q2) Performance comparison on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b379eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gpu must be present for this code to run\n",
    "if not torch.cuda.is_available():\n",
    "    logger.error(\"GPU is not available. Exiting the program.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd990c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_experiment_dir = './q2_experiments'\n",
    "os.makedirs(base_experiment_dir, exist_ok=True)\n",
    "\n",
    "search_space = {\n",
    "    \"dataset_name\": \"fashion-mnist\",\n",
    "    \"model_name\": [\"resnet18\", \"resnet50\"],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"batch_size\": [16],\n",
    "    \"optimizer_name\": [\"sgd\", \"adam\"],\n",
    "    \"num_epochs\": [1],\n",
    "    \"device\": ['cpu', 'cuda']\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name in search_space[\"model_name\"]:\n",
    "    for optimizer_name in search_space[\"optimizer_name\"]:\n",
    "        for device_type in search_space[\"device\"]:\n",
    "            config = {\n",
    "                \"dataset_name\": search_space[\"dataset_name\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"learning_rate\": search_space[\"learning_rate\"][0],\n",
    "                \"batch_size\": search_space[\"batch_size\"][0],\n",
    "                \"optimizer_name\": optimizer_name,\n",
    "                \"num_epochs\": search_space[\"num_epochs\"][0],\n",
    "                \"device\": device_type\n",
    "            }\n",
    "            device = torch.device(device_type)\n",
    "            logger.info(f\"Training with config: {config} on device: {device}\")\n",
    "            training_stats = train_model(\n",
    "                config, os.path.join(base_experiment_dir, f'{device_type}_experiments'),\n",
    "                device=device\n",
    "            )\n",
    "            training_stats = {}\n",
    "            logger.info(f\"Training completed.\\n\")\n",
    "            results.append({**config, **training_stats})\n",
    "    \n",
    "# calculate FLOPs for each configuration\n",
    "for result in results:\n",
    "    model_name = result[\"model_name\"]\n",
    "    if model_name == \"resnet18\":\n",
    "        model = get_resnet18_model().to('cpu')\n",
    "    elif model_name == \"resnet32\":\n",
    "        model = get_resnet32_model().to('cpu')\n",
    "    elif model_name == \"resnet50\":\n",
    "        model = get_resnet50_model().to('cpu')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "    model.to(result[\"device\"])\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(result[\"device\"])\n",
    "    total_flops = FlopCountAnalysis(model, dummy_input)\n",
    "    result['flops'] = total_flops.total()\n",
    "    print(f\"Model: {model_name}, Device: {result['device']}, FLOPs: {total_flops.total()}\")\n",
    "\n",
    "# save the results to json file\n",
    "with open(os.path.join(base_experiment_dir, 'device_comparison_experiments_results.json'), 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
